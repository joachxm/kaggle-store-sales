{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from collections.abc import Callable\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import tensorflow as tf\n",
    "from plotly.subplots import make_subplots\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss, MSE, MSLE\n",
    "\n",
    "print(\"TensorFlow version:\", tf.version.VERSION)\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)  # for nbviewer\n",
    "plotly.io.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_STEPS = 16  # length of the target sequences\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000  # for dataset shuffling\n",
    "\n",
    "# make a directory to store the trained models\n",
    "os.makedirs(\"models\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe created in the previous notebook\n",
    "df = pl.read_csv(os.path.join(\"input\", \"df.csv\"), try_parse_dates=True)\n",
    "\n",
    "df = df.cast({\"date\": pl.Float64, \"onpromotion\": pl.Float64})\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"store_nbr\").cast(pl.String).cast(pl.Categorical),\n",
    "        pl.col(\"family\").cast(pl.Categorical),\n",
    "        pl.col(\"store_type\").cast(pl.Categorical),\n",
    "        pl.col(\"store_cluster\").cast(pl.String).cast(pl.Categorical),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"store_type\", \"store_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('date', Float64),\n",
       "        ('sales', Float64),\n",
       "        ('onpromotion', Float64),\n",
       "        ('dcoilwtico', Float64),\n",
       "        ('transactions', Float64),\n",
       "        ('sin_wk', Float64),\n",
       "        ('cos_wk', Float64),\n",
       "        ('sin_mth', Float64),\n",
       "        ('cos_mth', Float64),\n",
       "        ('sin_yr', Float64),\n",
       "        ('cos_yr', Float64),\n",
       "        ('store_nbr', Categorical(ordering='physical')),\n",
       "        ('family', Categorical(ordering='physical'))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTensor:\n",
    "    def __init__(self, df: pl.DataFrame, split: tuple[float, float] = (0.7, 0.2)):\n",
    "        # one-hot encode categorical variables\n",
    "        self.df = df.to_dummies(cs.categorical())\n",
    "\n",
    "        dfs = self.df.partition_by(\n",
    "            cs.contains(\"store_nbr\", \"family\"), maintain_order=True\n",
    "        )\n",
    "\n",
    "        # stack into a tensor with axes (time, key, feature)\n",
    "        self.data = tf.stack([tf.constant(df, dtype=tf.float32) for df in dfs], axis=1)\n",
    "\n",
    "        self._compute_mean_std()\n",
    "        self._split(split)\n",
    "\n",
    "    def _split(self, split: tuple[float, float]):\n",
    "        train_ts = len(self.data) - TARGET_STEPS\n",
    "\n",
    "        # compute the number of time-steps in train/valid/test/target sets\n",
    "        split_steps = [int(train_ts * spl) for spl in split]\n",
    "        split_steps += [train_ts - sum(split_steps), TARGET_STEPS]\n",
    "\n",
    "        subset_name = [\"train\", \"valid\", \"test\", \"target\"]\n",
    "        subset_data = tf.split(self.data, split_steps, axis=0)\n",
    "        self.subsets = dict(zip(subset_name, subset_data))\n",
    "\n",
    "    def _compute_mean_std(self):\n",
    "        tmp_count = len(df.select(cs.float()).columns)  # number of temporal features\n",
    "        data, _ = tf.split(self.data, [-1, TARGET_STEPS], axis=0)  # remove target\n",
    "\n",
    "        # replace the categorical features with zeros to compute mean and std\n",
    "        tmp_data, cat_data = tf.split(data, [tmp_count, -1], axis=-1)\n",
    "        tmp_data = tf.concat([tmp_data, tf.zeros_like(cat_data)], axis=-1)\n",
    "\n",
    "        self.mean = tf.reduce_mean(tmp_data, axis=0, keepdims=True)\n",
    "        self.std = tf.math.reduce_std(tmp_data, axis=0, keepdims=True)\n",
    "        self.std = tf.where(self.std < 0.1, tf.ones_like(self.std), self.std)\n",
    "\n",
    "    def get(self, subset: str, norm: bool) -> tf.Tensor:\n",
    "        data = self.subsets[subset]\n",
    "        return (data - self.mean) / self.std if norm else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DataTensor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (0, 1178),\n",
       " 'valid': (1178, 1515),\n",
       " 'test': (1515, 1684),\n",
       " 'target': (1684, 1700)}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts = 1684\n",
    "\n",
    "idx = [0, int(0.7 * tts), int(0.9 * tts), tts, tts + TARGET_STEPS]\n",
    "idx\n",
    "d = dict(train=0, valid=1, test=2, target=3)\n",
    "{s: (idx[v], idx[v + 1]) for s, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def spl_split(xs: tf.Tensor, input_steps: int) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    # split the windows along the time axis into (head, tail)\n",
    "    head, tail = tf.split(xs, [input_steps, TARGET_STEPS], axis=1)\n",
    "\n",
    "    # TODO replace hard-coded value with variable\n",
    "    # extract the tail 'sales' values to use as label\n",
    "    tail_sales = tf.gather(tail, indices=[1], axis=-1)\n",
    "\n",
    "    return head, tail_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDatasets:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: DataTensor,\n",
    "        input_steps: int,\n",
    "        split_fn: Callable,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.input_steps = input_steps\n",
    "        self.window_steps = input_steps + TARGET_STEPS\n",
    "        self.split_fn = split_fn\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def make(self, subset: str) -> tf.data.Dataset:\n",
    "        # card = time-steps, spec = [keys, features]\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.data.get(subset, norm=True))\n",
    "\n",
    "        # card = windows, spec = [window_steps, keys, features]\n",
    "        ds = ds.window(size=self.window_steps, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda window: window.batch(self.window_steps))\n",
    "\n",
    "        # card = windows, spec = [keys, window_steps, features]\n",
    "        ds = ds.map(lambda xs: tf.transpose(xs, perm=[1, 0, 2]))\n",
    "\n",
    "        # card = windows * keys, spec = [window_steps, features]\n",
    "        ds = ds.flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "\n",
    "        ds = ds.shuffle(BUFFER_SIZE).batch(self.batch_size)\n",
    "        ds = ds.map(\n",
    "            lambda xs: self.split_fn(xs, self.input_steps),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        ds = ds.repeat().take(self.length(subset))  # set the cardinality\n",
    "\n",
    "        return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def length(self, subset: str) -> int:\n",
    "        shape = self.data.get(subset, norm=False).shape\n",
    "        windows_per_key = shape[0] - self.window_steps + 1\n",
    "        example_count = shape[1] * windows_per_key\n",
    "\n",
    "        return int(np.ceil(example_count / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wds = WindowDatasets(dt, input_steps=32, split_fn=spl_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 32, 98), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 16, 1), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wds.make(\"train\").element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Baseline model which used the past values as prediction\n",
    "# NOTE Match the day of the week since we know that there is a srong weekly periodicity.\n",
    "\n",
    "\n",
    "class Baseline(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"baseline\")\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        head_sales = tf.gather(inputs, [1], axis=-1)\n",
    "\n",
    "        # split along time axis with shift to match the weekdays\n",
    "        _, pred, _ = tf.split(head_sales, [-1, TARGET_STEPS, 5], axis=1)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3425/3425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 940us/step - loss: 1.7852\n"
     ]
    }
   ],
   "source": [
    "baseline = Baseline()\n",
    "baseline.compile(loss=\"mse\")\n",
    "\n",
    "baseline.evaluate(wds.make(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_dense = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(TARGET_STEPS),\n",
    "        keras.layers.Reshape([TARGET_STEPS, 1]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3425/3425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 13ms/step - loss: 1.7162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6424674987792969"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_dense.compile(optimizer=Adam(learning_rate=1e-5), loss=\"mse\")\n",
    "spl_dense.evaluate(wds.make(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy (including weights) to compare before/after training\n",
    "spl_dense_unt = keras.models.clone_model(spl_dense)\n",
    "spl_dense_unt.set_weights(spl_dense.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_dense.fit(x=wds.make(\"train\"), validation_data=wds.make(\"valid\"), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3425/3425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 1.2538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2257907390594482"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_dense.evaluate(wds.make(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_dense_model.save(\"byKey_simple_dense.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.LSTM(512, activation=\"relu\", return_sequences=False),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(TARGET_STEPS),\n",
    "        keras.layers.Reshape([TARGET_STEPS, 1]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: keras.Model, input_steps: int, split_fn: Callable, loss: Loss = MSLE\n",
    ") -> np.ndarray:\n",
    "    window_steps = input_steps + TARGET_STEPS\n",
    "\n",
    "    _, inputs = tf.split(dt.get(\"test\", norm=True), [-1, window_steps], axis=0)\n",
    "    _, target = tf.split(dt.get(\"test\", norm=False), [-1, TARGET_STEPS], axis=0)\n",
    "\n",
    "    # use key axis as batch -> (key, time, feature)\n",
    "    inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "    target = tf.transpose(target, [1, 0, 2])\n",
    "\n",
    "    target = tf.gather(target, indices=[1], axis=-1)  # select \"sales\" values\n",
    "    inputs, _ = spl_split(inputs, input_steps=input_steps)  # remove labels\n",
    "\n",
    "    preds = model(inputs)\n",
    "\n",
    "    sales_mean = tf.transpose(tf.gather(dt.mean, [1], axis=-1), [1, 0, 2])\n",
    "    sales_std = tf.transpose(tf.gather(dt.std, [1], axis=-1), [1, 0, 2])\n",
    "    preds = preds * sales_std + sales_mean\n",
    "\n",
    "    target = tf.squeeze(target)  # remove feature axis -> [TARGET_STEPS, keys]\n",
    "    preds = tf.squeeze(preds)\n",
    "\n",
    "    return loss(target, preds).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>baseline</th><th>spl_dense</th></tr><tr><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>1782.0</td><td>1782.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>0.400675</td><td>0.81705</td></tr><tr><td>&quot;std&quot;</td><td>1.474968</td><td>2.200993</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>0.054581</td><td>0.080969</td></tr><tr><td>&quot;50%&quot;</td><td>0.164079</td><td>0.258017</td></tr><tr><td>&quot;75%&quot;</td><td>0.421484</td><td>0.551733</td></tr><tr><td>&quot;max&quot;</td><td>24.951469</td><td>24.066505</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 3)\n",
       "┌────────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ baseline  ┆ spl_dense │\n",
       "│ ---        ┆ ---       ┆ ---       │\n",
       "│ str        ┆ f64       ┆ f64       │\n",
       "╞════════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 1782.0    ┆ 1782.0    │\n",
       "│ null_count ┆ 0.0       ┆ 0.0       │\n",
       "│ mean       ┆ 0.400675  ┆ 0.81705   │\n",
       "│ std        ┆ 1.474968  ┆ 2.200993  │\n",
       "│ min        ┆ 0.0       ┆ 0.0       │\n",
       "│ 25%        ┆ 0.054581  ┆ 0.080969  │\n",
       "│ 50%        ┆ 0.164079  ┆ 0.258017  │\n",
       "│ 75%        ┆ 0.421484  ┆ 0.551733  │\n",
       "│ max        ┆ 24.951469 ┆ 24.066505 │\n",
       "└────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.DataFrame(\n",
    "    {\n",
    "        \"baseline\": evaluate(baseline, input_steps=32, split_fn=spl_split),\n",
    "        \"spl_dense\": evaluate(spl_dense, input_steps=32, split_fn=spl_split),\n",
    "    }\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
