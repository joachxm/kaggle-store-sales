{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from collections.abc import Callable\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from plotly.subplots import make_subplots\n",
    "from tensorflow.keras.losses import MSLE, Loss\n",
    "\n",
    "print(\"TensorFlow version:\", tf.version.VERSION)\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)  # for nbviewer\n",
    "plotly.io.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_STEPS = 32  # length of input sequences (chosen)\n",
    "TARGET_STEPS = 16  # length of the target sequences (from competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe created in the previous notebook\n",
    "df = pl.read_csv(os.path.join(\"input\", \"df.csv\"), try_parse_dates=True)\n",
    "\n",
    "# cast the categorical columns to `pl.Categorical` (and hide the warning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"store_nbr\").cast(pl.String).cast(pl.Categorical),\n",
    "        pl.col(\"family\").cast(pl.Categorical),\n",
    "        pl.col(\"store_type\").cast(pl.Categorical),\n",
    "        pl.col(\"store_cluster\").cast(pl.String).cast(pl.Categorical),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that we will not use\n",
    "df = df.drop(\"transactions\", \"store_type\", \"store_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('date', Date),\n",
       "        ('sales', Float64),\n",
       "        ('onpromotion', Int64),\n",
       "        ('dcoilwtico', Float64),\n",
       "        ('sin_wk', Float64),\n",
       "        ('cos_wk', Float64),\n",
       "        ('sin_mth', Float64),\n",
       "        ('cos_mth', Float64),\n",
       "        ('sin_yr', Float64),\n",
       "        ('cos_yr', Float64),\n",
       "        ('store_nbr', Categorical(ordering='physical')),\n",
       "        ('family', Categorical(ordering='physical'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 3572)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE Pivot the dataframe to have one \"sales\" and \"onpromotion\" column per key\n",
    "# NOTE There are 1782 = (33 * 54) keys and 8 shared covariates, for a total of\n",
    "# NOTE 1782 * 2 + 8 = 3572 columns.\n",
    "\n",
    "pdf = df.pivot(\n",
    "    on=[\"store_nbr\", \"family\"],\n",
    "    index=[\"date\", \"dcoilwtico\", cs.contains(\"cos\", \"sin\")],\n",
    "    values=[\"sales\", \"onpromotion\"],\n",
    ")\n",
    "\n",
    "# store the indices of the \"sales\" columns and the shared covariates columns\n",
    "sales_idx = [pdf.get_column_index(col.name) for col in pdf.select(cs.contains(\"sales\"))]\n",
    "shared_idx = [\n",
    "    pdf.get_column_index(col.name)\n",
    "    for col in pdf.select(~cs.contains(\"sales\", \"onpromotion\"))\n",
    "]\n",
    "\n",
    "pdf.shape  # -> [time_steps, features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTensor:\n",
    "    def __init__(self, pdf: pl.DataFrame, split: tuple[float, float] = (0.7, 0.2)):\n",
    "        self.data = tf.constant(pdf, dtype=tf.float32)\n",
    "\n",
    "        train_data, _ = tf.split(self.data, [-1, TARGET_STEPS], axis=0)\n",
    "\n",
    "        # compute mean and std of training data for normalization\n",
    "        self.mean = tf.reduce_mean(train_data, axis=0, keepdims=True)\n",
    "        self.std = tf.math.reduce_std(train_data, axis=0, keepdims=True)\n",
    "        self.std = tf.where(self.std < 0.1, tf.ones_like(self.std), self.std)\n",
    "\n",
    "        # compute the number of time-steps in train/valid/test/target sets\n",
    "        split_steps = [int(len(train_data) * spl) for spl in split]\n",
    "        split_steps += [len(train_data) - sum(split_steps), TARGET_STEPS]\n",
    "\n",
    "        subset_name = [\"train\", \"valid\", \"test\", \"target\"]\n",
    "        subset_data = tf.split(self.data, split_steps, axis=0)\n",
    "        self.subsets = dict(zip(subset_name, subset_data))\n",
    "\n",
    "    def get(self, subset: str, norm: bool) -> tf.Tensor:\n",
    "        data = self.subsets[subset]\n",
    "        return (data - self.mean) / self.std if norm else data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DataTensor(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDatasets:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dt: DataTensor,\n",
    "        input_steps: int,\n",
    "        split: Callable,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        self._dt = dt\n",
    "        self.input_steps = input_steps\n",
    "        self.window_steps = input_steps + TARGET_STEPS\n",
    "        self._split = split\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def make(self, subset: str) -> tf.data.Dataset:\n",
    "        # build a dataset from the selected subset (normalized)\n",
    "        # -> card = time-steps, spec = [features]\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self._dt.get(subset, norm=True))\n",
    "\n",
    "        # window the dataset along the time axis\n",
    "        # -> card = windows, spec = [window_steps, features]\n",
    "        ds = ds.window(size=self.window_steps, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda window: window.batch(self.window_steps))\n",
    "\n",
    "        ds = ds.shuffle(1000).batch(self.batch_size)\n",
    "        ds = ds.map(\n",
    "            lambda xs: self._split(xs, self.input_steps),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        ds = ds.repeat().take(self.length(subset))  # set the cardinality\n",
    "\n",
    "        return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def length(self, subset: str) -> int:\n",
    "        time_steps = len(self._dt.get(subset, norm=True))\n",
    "        window_count = time_steps - self.window_steps + 1\n",
    "\n",
    "        return int(np.ceil(window_count / self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE We split a window along the time axis into two parts:\n",
    "# NOTE head (past time-steps) and tail (target time-steps).\n",
    "# NOTE The 'simple' models use only values from the head to predict the tail \"sales\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Pass the input steps for using in `split` instead of -1,\n",
    "# NOTE otherwise the element_spec does not show the number of time-steps.\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def spl_split(xs: tf.Tensor, input_steps: int) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    # split the windows along the time axis into (head, tail)\n",
    "    head, tail = tf.split(xs, [input_steps, TARGET_STEPS], axis=1)\n",
    "\n",
    "    # extract the tail \"sales\" values to use as label\n",
    "    tail_sales = tf.gather(tail, indices=sales_idx, axis=-1)\n",
    "\n",
    "    return head, tail_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 10, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_wds = WindowDatasets(dt, input_steps=INPUT_STEPS, split=spl_split)\n",
    "\n",
    "spl_wds.length(\"train\"), spl_wds.length(\"valid\"), spl_wds.length(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 32, 3572), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 16, 1782), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_wds.make(\"train\").element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Use the most recent values of the input while matching the weekdays.\n",
    "\n",
    "\n",
    "class Baseline(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, xs: tf.Tensor) -> tf.Tensor:\n",
    "        _, time_steps, _ = tf.split(xs, [-1, TARGET_STEPS, 5], axis=1)\n",
    "        vals = tf.gather(time_steps, indices=sales_idx, axis=-1)\n",
    "\n",
    "        return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.647246241569519"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = Baseline()\n",
    "baseline.compile(loss=\"mse\")\n",
    "baseline.evaluate(spl_wds.make(\"test\"), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_dense = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(TARGET_STEPS * 1782),\n",
    "        keras.layers.Reshape([TARGET_STEPS, 1782]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1824588775634766"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_dense.compile(optimizer=Adam(learning_rate=1e-4), loss=\"mse\")\n",
    "spl_dense.evaluate(spl_wds.make(\"test\"), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy (including weights) to compare before/after training\n",
    "spl_dense_unt = keras.models.clone_model(spl_dense)\n",
    "spl_dense_unt.set_weights(spl_dense.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_dense.fit(x=spl_wds.make(\"train\"), validation_data=spl_wds.make(\"valid\"), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.8754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8732141256332397"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_dense.evaluate(spl_wds.make(\"test\"), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_lstm = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.LSTM(256, activation=\"relu\", return_sequences=True),\n",
    "        keras.layers.LSTM(256, activation=\"relu\", return_sequences=False),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(TARGET_STEPS * 1782),\n",
    "        keras.layers.Reshape([TARGET_STEPS, 1782]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_lstm.compile(optimizer=Adam(learning_rate=1e-5), loss=\"mse\")\n",
    "spl_lstm.evaluate(spl_wds.make(\"test\"), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_lstm.fit(spl_wds.make(\"train\"), validation_data=spl_wds.make(\"valid\"), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy (including weights) to compare before/after training\n",
    "spl_lstm_unt = keras.models.clone_model(spl_lstm)\n",
    "spl_lstm_unt.set_weights(spl_lstm.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1.5825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5910042524337769"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl_lstm.evaluate(spl_wds.make(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coupling models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE \"Coupling\" models add an additional (dense) network whose role it is to couple the\n",
    "# NOTE output of a simple model with the known shared covariates on the window tail.\n",
    "# NOTE These models have two inputs and two (sequential) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Define a new split function adapted from the simple split function `spl_split`.\n",
    "# NOTE The difference is that we now return the shared covariates on the tail as part of the input.\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def cpl_split(\n",
    "    xs: tf.Tensor, input_steps: int\n",
    ") -> tuple[tuple[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "    # split the windows along the time axis into (head, tail)\n",
    "    head, tail = tf.split(xs, [input_steps, TARGET_STEPS], axis=1)\n",
    "\n",
    "    # extract the tail \"sales\" values to use as label\n",
    "    tail_sales = tf.gather(tail, indices=sales_idx, axis=-1)\n",
    "\n",
    "    # extract the tail shared covariates\n",
    "    tail_shared = tf.gather(tail, indices=shared_idx, axis=-1)\n",
    "\n",
    "    return (head, tail_shared), tail_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl_wds = WindowDatasets(dt, input_steps=INPUT_STEPS, split=cpl_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE The `spl_model` must be built BEFORE being passed to this function\n",
    "\n",
    "\n",
    "def make_cpl_model(spl_model: keras.Model) -> keras.Model:\n",
    "    # create a copy of the simple model to not modify the orignial\n",
    "    spl_model = keras.models.clone_model(spl_model)\n",
    "\n",
    "    # create the coupling network\n",
    "    coupnet = keras.Sequential(\n",
    "        [\n",
    "            # keras.layers.Dense(len(sales_idx), activation=\"relu\"),\n",
    "            keras.layers.Dense(len(sales_idx)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # create the two inputs: head (all features) and tail (shared features)\n",
    "    head = keras.Input(shape=[None, len(pdf.columns)], dtype=tf.float32)\n",
    "    tail = keras.Input(shape=[TARGET_STEPS, len(shared_idx)], dtype=tf.float32)\n",
    "\n",
    "    spl_preds = spl_model(head)\n",
    "    concat = keras.layers.Concatenate(axis=-1)([spl_preds, tail])\n",
    "    preds = coupnet(concat)\n",
    "\n",
    "    return keras.Model(inputs=(head, tail), outputs=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coupling dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl_dense = make_cpl_model(spl_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl_dense.compile(optimizer=Adam(learning_rate=1e-4), loss=\"mse\")\n",
    "cpl_dense.fit(cpl_wds.make(\"train\"), validation_data=cpl_wds.make(\"valid\"), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.7099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.700713038444519"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpl_dense.evaluate(cpl_wds.make(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Compute the loss per key for a given model on the last window of the test set (most recent data).\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: keras.Model, input_steps: int, split_fn: Callable, loss: Loss = MSLE\n",
    ") -> np.ndarray:\n",
    "    window_steps = input_steps + TARGET_STEPS\n",
    "\n",
    "    # get scaled and unscaled data of the last window of test set\n",
    "    _, inputs = tf.split(dt.get(\"test\", norm=True), [-1, window_steps], axis=0)\n",
    "    _, target = tf.split(dt.get(\"test\", norm=False), [-1, TARGET_STEPS], axis=0)\n",
    "\n",
    "    # select only the features corresponding to \"sales\" columns\n",
    "    target = tf.gather(target, indices=sales_idx, axis=-1)\n",
    "\n",
    "    # add batch axis and split\n",
    "    inputs, _ = split_fn(tf.expand_dims(inputs, axis=0), input_steps)\n",
    "\n",
    "    # predict and remove batch axis -> [TARGET_STEPS, keys]\n",
    "    preds = tf.squeeze(model(inputs))\n",
    "\n",
    "    # unscale the predictions\n",
    "    mean = tf.gather(dt.mean, indices=sales_idx, axis=-1)\n",
    "    std = tf.gather(dt.std, indices=sales_idx, axis=-1)\n",
    "    preds = preds * std + mean\n",
    "\n",
    "    # transpose to have [keys, TARGET_STEPS] and compute the loss\n",
    "    return loss(tf.transpose(preds), tf.transpose(target)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>baseline</th><th>spl_dense_unt</th><th>spl_dense</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mean&quot;</td><td>0.400675</td><td>1.152542</td><td>1.166112</td></tr><tr><td>&quot;std&quot;</td><td>1.474968</td><td>2.603318</td><td>4.145085</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>0.001854</td><td>0.000294</td></tr><tr><td>&quot;50%&quot;</td><td>0.164079</td><td>0.441517</td><td>0.320601</td></tr><tr><td>&quot;90%&quot;</td><td>0.740266</td><td>2.504551</td><td>2.082433</td></tr><tr><td>&quot;max&quot;</td><td>24.951469</td><td>41.689857</td><td>71.937698</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 4)\n",
       "┌───────────┬───────────┬───────────────┬───────────┐\n",
       "│ statistic ┆ baseline  ┆ spl_dense_unt ┆ spl_dense │\n",
       "│ ---       ┆ ---       ┆ ---           ┆ ---       │\n",
       "│ str       ┆ f64       ┆ f64           ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════════════╪═══════════╡\n",
       "│ mean      ┆ 0.400675  ┆ 1.152542      ┆ 1.166112  │\n",
       "│ std       ┆ 1.474968  ┆ 2.603318      ┆ 4.145085  │\n",
       "│ min       ┆ 0.0       ┆ 0.001854      ┆ 0.000294  │\n",
       "│ 50%       ┆ 0.164079  ┆ 0.441517      ┆ 0.320601  │\n",
       "│ 90%       ┆ 0.740266  ┆ 2.504551      ┆ 2.082433  │\n",
       "│ max       ┆ 24.951469 ┆ 41.689857     ┆ 71.937698 │\n",
       "└───────────┴───────────┴───────────────┴───────────┘"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = dict()\n",
    "\n",
    "losses[\"baseline\"] = evaluate(baseline, input_steps=INPUT_STEPS, split_fn=spl_split)\n",
    "\n",
    "losses[\"spl_dense_unt\"] = evaluate(\n",
    "    spl_dense_unt, input_steps=INPUT_STEPS, split_fn=spl_split\n",
    ")\n",
    "\n",
    "losses[\"spl_dense\"] = evaluate(spl_dense, input_steps=INPUT_STEPS, split_fn=spl_split)\n",
    "# losses[\"spl_lstm\"] = evaluate(spl_lstm, input_steps=INPUT_STEPS, split_fn=spl_split)\n",
    "\n",
    "# losses[\"cpl_dense\"] = evaluate(cpl_dense, input_steps=INPUT_STEPS, split_fn=cpl_split)\n",
    "\n",
    "pl.DataFrame(losses).describe([0.5, 0.9])[2:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
