{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from collections.abc import Callable\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"TensorFlow version:\", tf.version.VERSION)\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)  # for nbviewer\n",
    "plotly.io.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_STEPS = 16  # length of the target sequences\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000  # for dataset shuffling\n",
    "\n",
    "LOSS = keras.losses.MeanSquaredError()  # for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe created in the previous notebook\n",
    "df = pl.read_csv(os.path.join(\"input\", \"df.csv\"), try_parse_dates=True)\n",
    "\n",
    "# drop 'family' as 'family_nbr' contains the same information\n",
    "df = df.drop(\"family\")\n",
    "\n",
    "# cast 'onpromotion' to float to distinguish it from the categorical columns\n",
    "df = df.cast({\"onpromotion\": pl.Float64})\n",
    "\n",
    "# convert the 'date' column to a timestamp\n",
    "df = df.cast({\"date\": pl.Float64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('date', Float64),\n",
       "        ('sales', Float64),\n",
       "        ('transactions', Float64),\n",
       "        ('onpromotion', Float64),\n",
       "        ('dcoilwtico', Float64),\n",
       "        ('sin_hwk', Float64),\n",
       "        ('cos_hwk', Float64),\n",
       "        ('sin_wk', Float64),\n",
       "        ('cos_wk', Float64),\n",
       "        ('sin_mth', Float64),\n",
       "        ('cos_mth', Float64),\n",
       "        ('sin_yr', Float64),\n",
       "        ('cos_yr', Float64),\n",
       "        ('store_nbr', Int64),\n",
       "        ('family_nbr', Int64),\n",
       "        ('store_type', Int64),\n",
       "        ('store_cluster', Int64)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE Print the dataframe schema\n",
    "# NOTE The last four columns (integer) are the categorical columns which\n",
    "# NOTE depend on the key only.\n",
    "# NOTE Recall that we call 'key' the pairs (store_nbr, family_nbr).\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Store some values of the dataframe as constants\n",
    "\n",
    "FEATURES = len(df.columns)\n",
    "SALES_IDX = df.columns.index(\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE give names to the axes to avoid having numbers in the code\n",
    "\n",
    "\n",
    "class Ax:\n",
    "    TIME, KEY, FEATURE = 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Move `_norm` method outside of class?\n",
    "\n",
    "\n",
    "class DataTensor:\n",
    "    def __init__(self, df: pl.DataFrame, split: tuple[float, float] = (0.7, 0.2)):\n",
    "        # partition the dataframe by key (include_key=True by default)\n",
    "        # and stack the resulting dataframes into a tensor with axes determined by Ax\n",
    "        kdfs = df.partition_by(by=[\"store_nbr\", \"family_nbr\"], maintain_order=True)\n",
    "        kdfs = [tf.constant(kdf, dtype=tf.float32) for kdf in kdfs]\n",
    "        data = tf.stack(kdfs, axis=Ax.KEY)\n",
    "\n",
    "        # normalize the time- and key-features\n",
    "        time_data, keys_data = tf.split(data, [-1, 4], axis=Ax.FEATURE)\n",
    "        keys_data, _ = self._norm(keys_data, axis=Ax.KEY)\n",
    "        time_data, (mean, std) = self._norm(time_data, axis=Ax.TIME, tail=TARGET_STEPS)\n",
    "        data = tf.concat([time_data, keys_data], axis=Ax.FEATURE)\n",
    "\n",
    "        # store the mean and std of the 'sales' column for unscaling\n",
    "        sales_ind = df.columns.index(\"sales\")\n",
    "        self.mean = tf.gather(mean, [sales_ind], axis=Ax.FEATURE)\n",
    "        self.std = tf.gather(std, [sales_ind], axis=Ax.FEATURE)\n",
    "\n",
    "        # compute the number of time-steps in each subset\n",
    "        tts = data.shape[Ax.TIME] - TARGET_STEPS  # time-steps without target\n",
    "        steps = [int(tts * rt) for rt in split]  # train + valid\n",
    "        steps += [tts - sum(steps), TARGET_STEPS]  # test + target\n",
    "\n",
    "        # split the data into subsets and store those in `self._data`\n",
    "        train, valid, test, target = tf.split(data, steps, axis=Ax.TIME)\n",
    "        self._data = dict(train=train, valid=valid, test=test, target=target)\n",
    "\n",
    "    @classmethod\n",
    "    def _norm(cls, xs: tf.Tensor, axis: int, tail: int = 0):\n",
    "        head, _ = tf.split(xs, [-1, tail], axis=axis)\n",
    "\n",
    "        mean = tf.reduce_mean(head, axis=axis, keepdims=True)\n",
    "        std = tf.math.reduce_std(head, axis=axis, keepdims=True)\n",
    "        std = tf.where(std < 0.1, tf.ones_like(std), std)  # to avoid div. by ~0\n",
    "\n",
    "        return (xs - mean) / std, (mean, std)\n",
    "\n",
    "    def __getitem__(self, subset: str) -> tf.Tensor:\n",
    "        return self._data[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataTensor(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create windowed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Class that creates windowed datasets for the subsets.\n",
    "# NOTE Pass a split function which takes a batch of windows and splits them\n",
    "# NOTE into (inputs, label) pairs to match whatever our model expects.\n",
    "# NOTE Need to compute the length of each dataset manually since TensorFlow\n",
    "# NOTE cannot compute the cardinality due to `widnow`, and not knowing the\n",
    "# NOTE cardinality resluts in warnings when using `model.fit` and `model.evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDatasets:\n",
    "    def __init__(self, data: DataTensor, input_steps: int, split_fn: Callable):\n",
    "        self._data = data\n",
    "        self.input_steps = input_steps\n",
    "        self.window_steps = input_steps + TARGET_STEPS\n",
    "        self._split = split_fn\n",
    "\n",
    "    def make(self, subset: str) -> tf.data.Dataset:\n",
    "        # card = time-steps, spec = [keys, features]\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self._data[subset])\n",
    "\n",
    "        # card = windows, spec = [window_steps, keys, features]\n",
    "        ds = ds.window(size=self.window_steps, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda window: window.batch(self.window_steps))\n",
    "\n",
    "        # card = windows, spec = [keys, window_steps, features]\n",
    "        ds = ds.map(lambda xs: tf.transpose(xs, perm=[Ax.KEY, Ax.TIME, Ax.FEATURE]))\n",
    "\n",
    "        # card = windows * keys, spec = [window_steps, features]\n",
    "        ds = ds.flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "\n",
    "        ds = ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "        ds = ds.map(\n",
    "            lambda xs: self._split(xs, self.input_steps),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        ds = ds.repeat().take(self.length(subset))  # set the cardinality\n",
    "\n",
    "        return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def length(self, subset: str) -> int:\n",
    "        shape = self._data[subset].shape\n",
    "        windows_per_key = shape[Ax.TIME] - self.window_steps + 1\n",
    "        example_count = shape[Ax.KEY] * windows_per_key\n",
    "\n",
    "        return int(np.ceil(example_count / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE simple models = models using only head values as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Function that splits a batch of windows into (inputs, label) pairs.\n",
    "# NOTE use the terminology: window = head + tail\n",
    "# NOTE We pass the number of input steps as parameter (instead of using -1 in `split`)\n",
    "# NOTE so that the resulting tensor shape is known,\n",
    "# NOTE i.e. [input_steps, features] instead of [None, features] when passing -1.\n",
    "# NOTE This makes the input shape known to our models, which can initialize dense\n",
    "# NOTE layers without having to have the input shape specified.\n",
    "\n",
    "# TODO test with @tf.function -> no difference?\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def split_windows(xs: tf.Tensor, input_steps: int) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    # split the windows along the time axis into (head, tail)\n",
    "    head, tail = tf.split(xs, [input_steps, TARGET_STEPS], axis=1)\n",
    "\n",
    "    # extract the tail 'sales' values to use as label\n",
    "    tail_sales = tf.gather(tail, indices=[SALES_IDX], axis=-1)\n",
    "\n",
    "    return head, tail_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wds = WindowDatasets(data, input_steps=30, split_fn=split_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Baseline model which used the past values as prediction\n",
    "# NOTE Match the day of the week since we know that there is a srong weekly periodicity.\n",
    "\n",
    "\n",
    "class Baseline(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"baseline\")\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        head_sales = tf.gather(inputs, [0], axis=-1)\n",
    "\n",
    "        # split along time axis with shift to match the weekdays\n",
    "        _, pred, _ = tf.split(head_sales, [-1, TARGET_STEPS, 5], axis=1)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3481/3481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 629us/step - loss: 2.4790\n"
     ]
    }
   ],
   "source": [
    "baseline = Baseline()\n",
    "baseline.compile(loss=LOSS)\n",
    "\n",
    "\n",
    "_ = baseline.evaluate(wds.make(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dense_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Flatten(),\n",
    "        # keras.layers.Reshape([-1]),  # flatten\n",
    "        keras.layers.Dense(TARGET_STEPS),\n",
    "        keras.layers.Reshape([TARGET_STEPS, 1]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dense_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss=LOSS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3481/3481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 1.3036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2517348527908325"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_dense_model.evaluate(wds.make(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m31547/31547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 3ms/step - loss: 0.3413 - val_loss: 0.9319\n",
      "Epoch 2/5\n",
      "\u001b[1m31547/31547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 3ms/step - loss: 0.3200 - val_loss: 0.8388\n",
      "Epoch 3/5\n",
      "\u001b[1m31547/31547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - loss: 0.3134 - val_loss: 0.8775\n",
      "Epoch 4/5\n",
      "\u001b[1m31547/31547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - loss: 0.3096 - val_loss: 0.8899\n",
      "Epoch 5/5\n",
      "\u001b[1m31547/31547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - loss: 0.3078 - val_loss: 0.9064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18528eb70>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_dense_model.fit(x=wds.make(\"train\"), validation_data=wds.make(\"valid\"), epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
